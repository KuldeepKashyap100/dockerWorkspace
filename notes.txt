1.  All the instructions here are used to setting up the image
    image -> Template/ Blueprint for the container.
    container -> is layer on top of the image it is a running instance of image
    container does not copy over code and environment to new container, a container will use 
    the environment stored in an image and just adds an extra layer on top of it(ex- running node server process)

2.  FROM keyword
    FROM -> allows us to build our image up on another base image
    we can also build image from scratch but we need some kind of 
    OS or other tool in there which our code needs
    Just enter the image name it could be locally present on our system
    or on Docker Hub. When we run any container using this image for 
    the first time it will be downloaded and cached locally so that it won't be downloaded again 
    snippet: FROM node

3.  WORKDIR keyword
    by default all the commands will be run inside the working directory of the image
    and default working directory is root directory.
    all the commands will work realtive to this directory
    snippet: WORKDIR /app

4.  COPY keyword
    Now we have to tell docker which files present on our local should go on to the image
    COPY command is used for that.
    COPY . .  -> here we specify two paths, 
    first path is our path present in our local system(i.e currently pointing to path where Dockerfile lives)[Host file path]
    excluding the Dockerfile though
    second path [image/container] file path
    every image/container has its own file system that is detached from our file system
    snippet: COPY . /app

5.  RUN keyword
    After copying all the files we want to run a command in the image
    by default all the commands will be run inside the working directory of the image
    and default working directory is root directory
    snippet: RUN npm install

6.  EXPOSE keyword
    when we run the container it will not work though because container is isolated
    from our local environment and as a result it has its own internal network
    and when we listen to port in the node application inside our container
    the container does not expose that port to our local machine,
    so we won't be able to listen on that port just because something
    is listening inside our container.
    below command will let docker know that when this container is started
    we wanna expose a port to our local system
    it is only for documentation purposes, it actually does not do anything
    snippet: EXPOSE 3000

7.  CMD keyword
    we could have used RUN node server.js
    but this start the server when the image is being build
    we should use CMD instruction. the difference is that it will not
    be executed when the image is created but when the container is started based on that image.
    the syntax is little bit different it takes an array as input and we have to split our command
    if we don't specify CMD, CMD of base image will be executed.
    with no base image and no CMD we will get an error.
    snippet: CMD ["node", "server.js"]

8.  docker build
    now we have instructions to create an image
    and that's what build command does
    it tells docker to build a new custom image
    based on a docker file and we also have to specify
    the path where this docker file is located
    snippet: docker build .
    To set the name of a image, it is little different from container
    snippet: "docker build -t image_name:tag" 
    here tag defines a specialized image within a group of images
    combination both should always generate unique identifier

9.  After Build
    above command will create an image and will return image id
    now that image is created we can create an instance of it
    which we call container using run command and image id
    you don't always have to copy / write out the full id.
    You can also just use the first (few) character(s) of image_id - just enough to have a unique identifier.
    snippet: docker run image_id
    we can also run the container using image name 
    snippet: "docker run -p 3000:3000 --name new_container new_image:latest"
    To expose a port we have to use -p flag, which stands for publish
    it tells docker under which local port of our machine, 
    this docker container port should be accessible
    snippet: docker run -p local_port:container_exposed_port image_id

10. Stop the container
    snippet: docker stop container_name

11. Layer based architecture and caching
    images work on layer based architecture
    all the instructions are cacheable layers and docker cache the response of these layers
    and if rebuild again it will take split second because result of all the instructions
    is cached. but let's say we have 5 layers and user changed something so layer 3 needs to 
    rerun some instructions again now the below layer's will also not use the cache results but perform
    operations again.

12. Attached mode or Detached mode
    We can run the container either in attached mode or detached mode.
    by default, it is set to attached mode that means we will be able to
    see the output of container in console we can disable it by adding -d flag
    in run command. we can also use "docker attach container_name" to attach again
    to the running container.

13. LISTING COMMANDS
    to list all running containers -> "docker ps"
    to list all containers -> "docker ps -a"
    to list all images -> "docker images"
    ps stands for "process status"

14. REMOVE COMMANDS
    to remove container -> "docker rm container_name"
    we can remove container if it is not already running
    to remove images -> "docker rmi image_id"
    we can remove images if it is not being used by any container(wheather is running or not)
    to remove all containers that is not running -> "docker container prune"
    to remove all images that is not being used by any container -> "docker image prune"

15. CONTAINER START COMMANDS
    snippet: docker start container_name
    will start the already existing container
    run command creates a new container every time.
    To make the terminal on host interactive, we can use -i flag (that keeps the STDIN open)
    in combination with -t flag (that creates sudeo terminal).
    to name the containers while running 
    we can use --name flag to set the name
    to remove the container when it stops running, we can --rm flag when we start a container.


16. Copy contents out of the running container
    docker provides commands to copy files to and from the running container
    it is very error prone but 2 scenarios where it could be used is getting
    logs out of the running container or changing configuration files.
    snippet: "docker cp souce_dir_of_host container_name:/target_dir_inside_container"

17. Temporary and Permanent data 
    file system for host machine, image, container are all 3 different
    Images are read only (we rebuild image after every change so that our updated changes goes into images file system)
    Container is layer on top of image is read-write(ex- we may generate a file on container running server 
    which will contantly storing logs.)

18. Two types of external data storages
    Volumes -  managed by docker
    Bind Mounts - managed by you

19. Volumes
    problem: when we remove containers all the read-write data is removed along with the container
    solution: docker has a built in feature called Volumes and volumes help us persist the data
    volumes are folders in host machine hard drive which are mounted(mapped or made available) into the containers
    volumes persist if a container shuts down if a container restarts and mounts a volume
    any data inside of that volume is available in the container
    a container can read and write data from a volume

20. Two types of volumes
    Anonymous volumes - it does not have any name and it exists as long as our container exists
    Named volumes - 
    anonymous volumes are closely attached with the container
    In both types docker sets up a folder/path on your local machine
    exact location is unknown to you
    Managed via docker volume commands 
    To List all volumes
    snippet: docker volume ls

21. Removing Anonymous Volumes
    We saw, that anonymous volumes are removed automatically, when a container is removed.
    This happens when you start / run a container with the --rm option.
    If you start a container without that option, the anonymous volume would NOT be removed, 
    even if you remove the container (with docker rm ...).
    Still, if you then re-create and re-run the container (i.e. you run docker run ... again), 
    a new anonymous volume will be created. So even though the anonymous volume wasn't removed automatically, 
    it'll also not be helpful because a different anonymous volume is attached the next time the container starts 
    (i.e. you removed the old container and run a new one).
    Now you just start piling up a bunch of unused anonymous volumes 
    - you can clear them via docker volume rm VOL_NAME or docker volume prune.


21. Named Volume
    a defined path in the container is mapped to the created volume/mount
    ex- some path on your hosting machine mapped to the /app/data
    (With named volumes, volumes will survive container shutdown)
    Name Volumes is great for the data that should be persistent but which you don't need to edit directly
    because you don't have access to that folder on you host machine
    snippet: docker run 3000:3000 -v host_path:container_path image_name
    ex- docker run 3000:3000 -v feedback:/app/feedback image_name

22. Bind Mounts 
    problem: during development we change code a lot but we have to rebuild image every time
    for the changes to get reflected that's where bind mounts can help us
    bind mounts have some similarities with volumes but there is one key difference
    where volumes are managed by docker and we don't know where on our host machine they are.
    for bind mounts we do know it.
    Because for bind mounts we as a developer set the path to which the container internal path
    should be mapped on our host machine.
    container cannot just write from volumes but also read from them, 
    ofcourse we could put out source code into such bind mount and if we do that
    we could then make sure that container is aware of that and the souce is actually not used
    from that copied in snapshot but instead from bind mount
    so bind mounts are perfect for persistent and editable data.
    snippet: docker run 3000:3000 -v "absolute_project_path:/app image_name"
    ex: docker run 3000:3000 -v "/Users/kuldeep/dev/nodeProject:/app" image_name
    note- If you don't always want to copy and use the full path, you can use these shortcuts:
    macOS / Linux: -v $(pwd):/app
    Windows: -v "%cd%":/app

23: To check errors while running container
    snippet: docker logs container_name

24. Combining and merging different volumes
    we have a container and we have volume and bind mount
    we can mount both into the container
    that means some folders inside of the container are mounted or connected
    to folders on the host machine 
    now let's say we already had files inside of the container
    in that case they also now exists outside on the outside volume
    and if you write a new file, it's also added in the folder on your host machine
    and if the container then starts up and it finds files in the volume
    and it doesn't have any internal files yet, it loads the files from the volume
    that's what we utilize with the bind mount
    here we don't have any files inside of the container, let's say,
    but we have files on the local host machine 
    In that case these files are basically also usable inside the container
    note- If we mount bind mount, data present in the container will overwritten by bind mount data
    ex- we can copy all the project dir to container and install node_modules but we can also bind mount
    current project to container in that case node_modules will be removed.
    To solve the above problem we have to explicitly tell docker that there are certain parts in
    its internal file system that which should not be overwritten from outside in case we have a clash
    and that can be achieved with another anonymous volume which we add to this docker container
    this is also a use case of anonymous volume
    snippet: docker run 3000:3000 -v "/Users/kuldeep/dev/nodeProject:/app" -v "/app/node_modules" image_name
    now why does anonymous volume helped here?
    docker always evaluates all volumes you are setting on the container
    and if there are clashes, the longer internal path wins

25. Read only volumes
    By Default volumes are read/write.
    which means container can read from them and write data to them
    but we can restrict that by adding extra colon and add RO
    snippet:  docker run 3000:3000 -v "/Users/kuldeep/dev/nodeProject:/app:RO" image_name
    this means is container won't be able to write data in /app folder but we still be able to

26. To see the list of all the volumes
    snippet: docker volume ls

27. To see the path of anonymous volume
    snippet: docker volume inspect volume_name
    it not the path in your host machine but in the virtual machine setup by docker

28. ARGuments variables
    Docker supports built-time ARGuments and runtime ENVironment variables
    ARGuments variables - allows you to set flexible bit of data(i.e variables) 
    in you docker file which you can use in there to pluck different values
    into certain different docker instructions based on arguments that are provided
    with the --build-arg option when you run docker build
    set on image build via --build-arg on docker build
    In docker file
    snippet: ARG DEFAULT_PORT=80
             ENV PORT $DEFAULT_PORT
             EXPOSE $PORT
    During docker build
    snippet: docker build --build-arg DEFAULT_PORT=8000 .

29. ENVironment variables
    ENVironment variables - are available inside the of Dockerfile like arg
    but also available in your entire application code 
    you can set them via --env on docker run 
    In docker file
    snippet: ENV PORT=80
             EXPOSE $PORT
    During docker run
    snippet: docker run --env PORT=8000 image_name
    now this port will be available in process.ENV in node
    we can also create a file named .env and in this file you can setup you env variables
    inside .env -> PORT=8000
    now during run you have to specify this .env file
    snippet: docker run --env-file ./.env image_name

30. Environment Variables & Security
    One important note about environment variables and security: Depending on which kind of data 
    you're storing in your environment variables, 
    you might not want to include the secure data directly in your Dockerfile.
    Instead, go for a separate environment variables file which is then only used at runtime 
    (i.e. when you run your container with docker run).
    Otherwise, the values are "baked into the image" and everyone can read these values via docker history <image>.
    For some values, this might not matter but for credentials, private keys etc. you definitely want to avoid that!
    If you use a separate file, the values are not part of the image since you point at that file 
    when you run docker run. But make sure you don't commit 
    that separate file as part of your source control repository, 
    if you're using source control.

31. Making container to host communication work
    special domain to connect to local running server "host.docker.internal"
    this is understood by docker and translated to your host machine
    snippet: mongoose.connect("mongodb://host.docker.internal:27017/swfavorites", {}, () => {})

32. Creating container networks
    with docker we can create container networks
    the idea is that you might have multiple containers and you want 
    to allow communication b/w these containers and now with docker
    you can put all of these container into one and the same network
    by adding the --network option on docker run command
    this then creates a network in which all the containers are able to talk to each other
    and IPs are automatically resolved 
    it's a really useful feature for having multiple, isolated containers with their
    own duties and tasks and still able to talk to each other
    snippet: docker run --network network_name container_name_1 container_name_2
    note- unlike volume you have to create network, docker will not do that for you
    snippet: docker network create network_name
    Now if database server is running in container_name_2 then
    we container_name_1 can talk to it something like this
    snippet: mongoose.connect("mongodb://container_name_2:27017/swfavorites", {}, () => {})

    Note- Docker will not replace your source code it simply detects outgoing requests
    and resolve the IP address for such requests

33. Docker Network Drivers
    Docker Networks actually support different kinds of "Drivers" which influence the behavior of the Network.
    The default driver is the "bridge" driver - it provides the behavior 
    shown in this module (i.e. Containers can find each other by name if they are in the same Network).
    The driver can be set when a Network is created, simply by adding the --driver option.
    snippet: docker network create --driver bridge my-net
    Of course, if you want to use the "bridge" driver, 
    you can simply omit the entire option since "bridge" is the default anyways.
    Docker also supports these alternative drivers - though you will use the "bridge" driver in most cases:
    host: For standalone containers, isolation between container 
    and host system is removed (i.e. they share localhost as a network)
    overlay: Multiple Docker daemons (i.e. Docker running on different machines) are 
    able to connect with each other. Only works in "Swarm" mode which is a dated / almost 
    deprecated way of connecting multiple containers
    macvlan: You can set a custom MAC address to a container - this address can 
    then be used for communication with that container
    none: All networking is disabled.
    Third-party plugins: You can install third-party plugins which then may add all kinds of behaviors and functionalities
    As mentioned, the "bridge" driver makes most sense in the vast majority of scenarios.

34. docker-compose
    docker compose is a tool that allows you to replace
    docker-build and docker-run commands with just one 
    configuration file and then a set of orchestration commands
    to start all those services, all the containers at once 
    and build all necessary images, if it should be required
    and then you can also use one command to stop everything 
    and bring every thing down
    this approach is great for multicontainer application
    snippet: docker-compose.yaml
        version: 3.8 (version helps us to tell docker what features we are going to use)
        services:    (services are basically containers)
            mongodb:
                image: 'mongo'             
                # image we want to use for this container
                volumes: 
                    - data:/data/db        
                    #for every volume add a dash
                environment:
                    MONGO_USER: root
                    MONGO_PASSWORD: secret or - MONGO_PASSWORD=secret
                env_file: 
                    - ./mongo.env
                # we don't have to specify --rm flag by default container will be removed on shutdown)
                container_name: mongodb
            backend:
                build: ./backend    
                # or 
                # build: 
                #    context: ./backend   (set the context in which docker file will be built)
                #    dockerfile: Dockerfile
                #    args:
                #       some-args: 1 
                # docker compose also  replaces image-build step instead of specifying finished 
                # image we can give docker compose all the information it needs to build an image
                # now the build option wants to know where to find the docker file that should be built

                volumes:
                    - logs:/app/logs
                    - ./backend:/app
                    - /app/node_modules

                ports: 
                    - '80:80'

                depends_on:    
                    - mongodb
                # we can specify the list of other containers
                # on which this container depends on so that docker can first 
                # bring up those container and then this one
                
                # docker-compose creates a default network and adds all containers to that network

            frontend:
                build: ./frontend
                ports: 
                    - '3000:3000'
                stdin_open: true
                # to let the docker know that this service needs a open input connection 
                tty: true
                # tty flag to attach terminal to standard input connection
                # both of the above two keys are equivalent to -it flag
            volumes:      (all the named volumes used above have to be listed here)
                data:     (no value needs to be assigned just list them here)
                logs:
            

                 
35. To start services mentioned in docker-compose
    and it will not just start the containers 
    it will also pull and build all the images that might be required
    snippet: docker-compose up (build + run)
             docker-compose build (build)

36. If we want to shut and remove all containers
    it does not deletes volumes though if you want to do that add -v flag
    snippet: docker-compose down

37. Utility containers
    these containers only have certain environment in them.
    let's say a nodejs or php environment something like that.
    The idea is that they don't start an application
    when you run them but instead you run them in conjunction with some 
    command specified by you to then execute certain task 
    snippet: docker run mynpm init
    this is special syntax adding a extra command after image name
    snippet: 
    docker run -it -d node
    docker exec -it container_name npm init

    this can be useful if want to read logs from running containers

38. We can setup ec2 instance on aws and use ssh(secure shell)
    to communicate to this remote machine

39. Pushing our local image to the cloud
    we can prepare our image locally and then just get the finished
    image onto the remote host to then run it as a container
    and we can use docker hub for that.
    we can build our image locally then push it to docker hub
    and then pull it from remote server to then run it on the remote server.
    first create new repository on docker hub
    now we can push to this repository by creating an image on our local machine
    snippet:
    docker build local_image_name . 
    docker tag local_image_name kuldeepkashyap/temp-repo
    docker push kuldeepkashyap/temp-repo

40. Now that image is pushed we have to now and run it on remote machine.
    snippet: docker run -d --rm kuldeepkashyap/temp-repo
    we didn't need to configure node.js environment on the server
    but we just installed docker and used it run the locally build image.

41. To do any changes in container running on remote machine,
    do the changes on your local and rebuild the image and add appropriate tag,
    and push it to docker hub.
    Now on the remote machine stop the currently running container, 
    pull the updated image and run it.
    snippet: 
    docker build local_image_name . 
    docker tag local_image_name kuldeepkashyap/temp-repo
    docker push kuldeepkashyap/temp-repo

    docker stop container_name
    docker pull kuldeepkashyap/temp-repo
    docker run -d --rm kuldeepkashyap/temp-repo

42. Disadvantages of current approach
    you fully own ec2 machine, you are fully responsible for it and also for its security
    you are responsible for configuration of this remote machine
    you have to take care if it's powerful enough
    you have to replace it with more powerful one if you get more traffic.
    you also have to managed in which network it is.
    the security groups and therefore firewall
    solution: Managed remote machines(AWS ECS) (elastic container service)

43. Configuring ECS has 4 steps
    container definition: configure all the containers that our application going to use
    task definition: a blueprint for you application here we can tell aws how it should launch our Container
        not how it should execute docker run but how the server on which it runs this should be configured
        here we are telling aws how it should execute our containers and which environment it should
        generally setup for them.
        fargate(default): which lauches it in a serverless-mode which means aws does not create an ec2 instance
        but instead it stores your container and your run settings and whenever there is a request that requires 
        the container to do something it starts the container up and handles that request and then stops it again
        which ofcourse is cost effective because you pay for the time your container is executing.
    service: controls the configured application should be executed 
            ex - you can add load balancer here
    cluster: a overall network in which services is run 
    
44. EFS(elastic file system)
    EFS allows us to attach a file system to our containers
    we have to configure volume or bind mount that container is using EFS.
    EFS allows us to attach hard drives to containers which will survive 
    even if these containers are redeployed.

45. Multi-stage builds
    allows you to have one dockerfile that define multiple build steps
    or setup steps so called stages inside of that file
    stages can copy results(created files and folders) from each other 
    so we can have one stage to create the optimized files
    and another stage to serve them.
    we can either build the complete image or select individual stages
    Every "FROM" instruction creates a new stage in your docker file 
    we must have only one base image but we can switch that image
    in that case the previous steps will be discarded and we will
    switch to new base image.
    but there is a special keyword("AS") if we don't want our changes to be discarded.
    snippet:
    FROM node:14-alpine as frontendApp
    WORKDIR /app
    COPY package.json .
    RUN npm install
    COPY . .
    RUN npm build

    FROM nginx:stable-alpine
    COPY --from=frontendApp /app/build /usr/share/nginx/html
    EXPOSE 80
    CMD ["nginx", "-g", "daemon off;"]

46. we can also stop the build at particular stage using --target flag.
    this is useful in more complex projects
    snippet: docker build --target frontendApp -f frontend/Dockerfile.prod ./frontend
    this will stop the build at "RUN npm build"

47. 3 ways to configure aws service
    -> by using visual user interface 
    -> by using command line tool( ecs-cli )
    -> by using configuration files(YAML) which aws reads and make modifications


Kubernetes

48. Kubernetes
    is more like a framework(a collection of tools), a collection of concepts, a standard
    which will help us with container orchestration and large scale deployments
    independent of the cloud provider we might be using

49. Problems with current deployment process
    -> we have to configure ec2 on our own and we have to take care of OS update and security
    -> container might crash/go down and need to be replaced with a new container
    -> we might need more container instances upon traffic spikes
    -> incoming traffic should be distributed evenly b/w container instances
    
50. Advantages of using ECS
    -> ECS monitor health of our containers and restarts them when they crash
    -> ECS also helps us with autoscaling
    -> ECS also have a tool called loadbalancer to distribute traffic evenly
    

51. Disadvantages of using ECS 
    if we use a specific cloud service by some cloud provider
    we are basically locked into that service 
    which means if we want to switch to another service provided by another cloud provider
    the config files we wrote for aws ecs won't work there becasue that is a aws specific config
    and we have to learn first about ECS and if want to switch learn about another service
    and that's where kubernetes can help us

52. why kubernetes?
    kubernetes can help us with the problem mentioned above
    with kubernetes we have a way of defining our deployments, 
    our scaling of containers 
    and how containers should be monitored and replaced if they fail
    we have a way of defining all of that independent of the cloud service we are using
    because kubernetes is an open source standard and de-facto standard for managing container deployments
    and for orchestrating containers and it can help us with tasks like automatic deployment
    with scaling and load balancing and with managing our deployments
    kubernetes simply allows us to write down a kubernetes configuration
    where we define our desired deployment which container we want to deploy
    how many instances, if it should scale up, if they should replaced and so on
    