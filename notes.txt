1.  All the instructions here are used to setting up the image
    image -> Template/ Blueprint for the container.
    container -> is layer on top of the image it is a running instance of image
    container does not copy over code and environment to new container, a container will use 
    the environment stored in an image and just adds an extra layer on top of it(ex- running node server process)

2.  FROM keyword
    FROM -> allows us to build our image up on another base image
    we can also build image from scratch but we need some kind of 
    OS or other tool in there which our code needs
    Just enter the image name it could be locally present on our system
    or on Docker Hub. When we run any container using this image for 
    the first time it will be downloaded and cached locally so that it won't be downloaded again 
    snippet: FROM node

3.  WORKDIR keyword
    by default all the commands will be run inside the working directory of the image
    and default working directory is root directory.
    all the commands will work realtive to this directory
    snippet: WORKDIR /app

4.  COPY keyword
    Now we have to tell docker which files present on our local should go on to the image
    COPY command is used for that.
    COPY . .  -> here we specify two paths, 
    first path is our path present in our local system(i.e currently pointing to path where Dockerfile lives)[Host file path]
    excluding the Dockerfile though
    second path [image/container] file path
    every image/container has its own file system that is detached from our file system
    snippet: COPY . /app

5.  RUN keyword
    After copying all the files we want to run a command in the image
    by default all the commands will be run inside the working directory of the image
    and default working directory is root directory
    snippet: RUN npm install

6.  EXPOSE keyword
    when we run the container it will not work though because container is isolated
    from our local environment and as a result it has its own internal network
    and when we listen to port in the node application inside our container
    the container does not expose that port to our local machine,
    so we won't be able to listen on that port just because something
    is listening inside our container.
    below command will let docker know that when this container is started
    we wanna expose a port to our local system
    it is only for documentation purposes, it actually does not do anything
    snippet: EXPOSE 3000

7.  CMD keyword
    we could have used RUN node server.js
    but this start the server when the image is being build
    we should use CMD instruction. the difference is that it will not
    be executed when the image is created but when the container is started based on that image.
    the syntax is little bit different it takes an array as input and we have to split our command
    if we don't specify CMD, CMD of base image will be executed.
    with no base image and no CMD we will get an error.
    snippet: CMD ["node", "server.js"]

8.  docker build
    now we have instructions to create an image
    and that's what build command does
    it tells docker to build a new custom image
    based on a docker file and we also have to specify
    the path where this docker file is located
    snippet: docker build .
    To set the name of a image, it is little different from container
    snippet: "docker build -t image_name:tag" 
    here tag defines a specialized image within a group of images
    combination both should always generate unique identifier

9.  After Build
    above command will create an image and will return image id
    now that image is created we can create an instance of it
    which we call container using run command and image id
    you don't always have to copy / write out the full id.
    You can also just use the first (few) character(s) of image_id - just enough to have a unique identifier.
    snippet: docker run image_id
    we can also run the container using image name 
    snippet: "docker run -p 3000:3000 --name new_container new_image:latest"
    To expose a port we have to use -p flag, which stands for publish
    it tells docker under which local port of our machine, 
    this docker container port should be accessible
    snippet: docker run -p local_port:container_exposed_port image_id

10. Stop the container
    snippet: docker stop container_name

11. Layer based architecture and caching
    images work on layer based architecture
    all the instructions are cacheable layers and docker cache the response of these layers
    and if rebuild again it will take split second because result of all the instructions
    is cached. but let's say we have 5 layers and user changed something so layer 3 needs to 
    rerun some instructions again now the below layer's will also not use the cache results but perform
    operations again.

12. Attached mode or Detached mode
    We can run the container either in attached mode or detached mode.
    by default, it is set to attached mode that means we will be able to
    see the output of container in console we can disable it by adding -d flag
    in run command. we can also use "docker attach container_name" to attach again
    to the running container.

13. LISTING COMMANDS
    to list all running containers -> "docker ps"
    to list all containers -> "docker ps -a"
    to list all images -> "docker images"
    ps stands for "process status"

14. REMOVE COMMANDS
    to remove container -> "docker rm container_name"
    we can remove container if it is not already running
    to remove images -> "docker rmi image_id"
    we can remove images if it is not being used by any container(wheather is running or not)
    to remove all containers that is not running -> "docker container prune"
    to remove all images that is not being used by any container -> "docker image prune"

15. CONTAINER START COMMANDS
    snippet: docker start container_name
    will start the already existing container
    run command creates a new container every time.
    To make the terminal on host interactive, we can use -i flag (that keeps the STDIN open)
    in combination with -t flag (that creates sudeo terminal).
    to name the containers while running 
    we can use --name flag to set the name
    to remove the container when it stops running, we can --rm flag when we start a container.


16. Copy contents out of the running container
    docker provides commands to copy files to and from the running container
    it is very error prone but 2 scenarios where it could be used is getting
    logs out of the running container or changing configuration files.
    snippet: "docker cp souce_dir_of_host container_name:/target_dir_inside_container"

17. Temporary and Permanent data 
    file system for host machine, image, container are all 3 different
    Images are read only (we rebuild image after every change so that our updated changes goes into images file system)
    Container is layer on top of image is read-write(ex- we may generate a file on container running server 
    which will contantly storing logs.)

18. Two types of external data storages
    Volumes -  managed by docker
    Bind Mounts - managed by you

19. Volumes
    problem: when we remove containers all the read-write data is removed along with the container
    solution: docker has a built in feature called Volumes and volumes help us persist the data
    volumes are folders in host machine hard drive which are mounted(mapped or made available) into the containers
    volumes persist if a container shuts down if a container restarts and mounts a volume
    any data inside of that volume is available in the container
    a container can read and write data from a volume

20. Two types of volumes
    Anonymous volumes - it does not have any name and it exists as long as our container exists
    Named volumes - 
    anonymous volumes are closely attached with the container
    In both types docker sets up a folder/path on your local machine
    exact location is unknown to you
    Managed via docker volume commands 
    To List all volumes
    snippet: docker volume ls

21. Removing Anonymous Volumes
    We saw, that anonymous volumes are removed automatically, when a container is removed.
    This happens when you start / run a container with the --rm option.
    If you start a container without that option, the anonymous volume would NOT be removed, 
    even if you remove the container (with docker rm ...).
    Still, if you then re-create and re-run the container (i.e. you run docker run ... again), 
    a new anonymous volume will be created. So even though the anonymous volume wasn't removed automatically, 
    it'll also not be helpful because a different anonymous volume is attached the next time the container starts 
    (i.e. you removed the old container and run a new one).
    Now you just start piling up a bunch of unused anonymous volumes 
    - you can clear them via docker volume rm VOL_NAME or docker volume prune.


21. Named Volume
    a defined path in the container is mapped to the created volume/mount
    ex- some path on your hosting machine mapped to the /app/data
    (With named volumes, volumes will survive container shutdown)
    Name Volumes is great for the data that should be persistent but which you don't need to edit directly
    because you don't have access to that folder on you host machine
    snippet: docker run 3000:3000 -v host_path:container_path image_name
    ex- docker run 3000:3000 -v "feedback:/app/feedback" image_name

22. Bind Mounts 
    problem: during development we change code a lot but we have to rebuild image every time
    for the changes to get reflected that's where bind mounts can help us
    bind mounts have some similarities with volumes but there is one key difference
    where volumes are managed by docker and we don't know where on our host machine they are.
    for bind mounts we do know it.
    Because for bind mounts we as a developer set the path to which the container internal path
    should be mapped on our host machine.
    container cannot just write from volumes but also read from them, 
    ofcourse we could put out source code into such bind mount and if we do that
    we could then make sure that container is aware of that and the souce is actually not used
    from that copied in snapshot but instead from bind mount
    so bind mounts are perfect for persistent and editable data.
    snippet: docker run 3000:3000 -v "absolute_project_path:/app" image_name
    ex: docker run 3000:3000 -v "/Users/kuldeep/dev/nodeProject:/app" image_name
    note- If you don't always want to copy and use the full path, you can use these shortcuts:
    macOS / Linux: -v $(pwd):/app
    Windows: -v "%cd%":/app

23: To check errors while running container
    snippet: docker logs container_name

24. Combining and merging different volumes
    we have a container and we have volume and bind mount
    we can mount both into the container
    that means some folders inside of the container are mounted or connected
    to folders on the host machine 
    now let's say we already had files inside of the container
    in that case they also now exists outside on the outside volume
    and if you write a new file, it's also added in the folder on your host machine
    and if the container then starts up and it finds files in the volume
    and it doesn't have any internal files yet, it loads the files from the volume
    that's what we utilize with the bind mount
    here we don't have any files inside of the container, let's say,
    but we have files on the local host machine 
    In that case these files are basically also usable inside the container
    note- If we mount bind mount, data present in the container will overwritten by bind mount data
    ex- we can copy all the project dir to container and install node_modules but we can also bind mount
    current project to container in that case node_modules will be removed.
    To solve the above problem we have to explicitly tell docker that there are certain parts in
    its internal file system that which should not be overwritten from outside in case we have a clash
    and that can be achieved with another anonymous volume which we add to this docker container
    this is also a use case of anonymous volume
    snippet: docker run 3000:3000 -v "/Users/kuldeep/dev/nodeProject:/app" -v "/app/node_modules" image_name
    now why does anonymous volume helped here?
    docker always evaluates all volumes you are setting on the container
    and if there are clashes, the longer internal path wins

25. Read only volumes
    By Default volumes are read/write.
    which means container can read from them and write data to them
    but we can restrict that by adding extra colon and add RO
    snippet:  docker run 3000:3000 -v "/Users/kuldeep/dev/nodeProject:/app:RO" image_name
    this means is container won't be able to write data in /app folder but we still be able to

26. To see the list of all the volumes
    snippet: docker volume ls

27. To see the path of anonymous volume
    snippet: docker volume inspect volume_name
    it not the path in your host machine but in the virtual machine setup by docker

28. ARGuments variables
    Docker supports built-time ARGuments and runtime ENVironment variables
    ARGuments variables - allows you to set flexible bit of data(i.e variables) 
    in you docker file which you can use in there to pluck different values
    into certain different docker instructions based on arguments that are provided
    with the --build-arg option when you run docker build
    set on image build via --build-arg on docker build
    In docker file
    snippet: ARG DEFAULT_PORT=80
             ENV PORT $DEFAULT_PORT
             EXPOSE $PORT
    During docker build
    snippet: docker build --build-arg DEFAULT_PORT=8000 .

29. ENVironment variables
    ENVironment variables - are available inside the of Dockerfile like arg
    but also available in your entire application code 
    you can set them via --env on docker run 
    In docker file
    snippet: ENV PORT=80
             EXPOSE $PORT
    During docker run
    snippet: docker run --env PORT=8000 image_name
    now this port will be available in process.ENV in node
    we can also create a file named .env and in this file you can setup you env variables
    inside .env -> PORT=8000
    now during run you have to specify this .env file
    snippet: docker run --env-file ./.env image_name

30. Environment Variables & Security
    One important note about environment variables and security: Depending on which kind of data 
    you're storing in your environment variables, 
    you might not want to include the secure data directly in your Dockerfile.
    Instead, go for a separate environment variables file which is then only used at runtime 
    (i.e. when you run your container with docker run).
    Otherwise, the values are "baked into the image" and everyone can read these values via docker history <image>.
    For some values, this might not matter but for credentials, private keys etc. you definitely want to avoid that!
    If you use a separate file, the values are not part of the image since you point at that file 
    when you run docker run. But make sure you don't commit 
    that separate file as part of your source control repository, 
    if you're using source control.

31. Making container to host communication work
    special domain to connect to local running server "host.docker.internal"
    this is understood by docker and translated to your host machine
    snippet: mongoose.connect("mongodb://host.docker.internal:27017/swfavorites", {}, () => {})

32. Creating container networks
    with docker we can create container networks
    the idea is that you might have multiple containers and you want 
    to allow communication b/w these containers and now with docker
    you can put all of these container into one and the same network
    by adding the --network option on docker run command
    this then creates a network in which all the containers are able to talk to each other
    and IPs are automatically resolved 
    it's a really useful feature for having multiple, isolated containers with their
    own duties and tasks and still able to talk to each other
    snippet: docker run --network network_name container_name_1 container_name_2
    note- unlike volume you have to create network, docker will not do that for you
    snippet: docker network create network_name
    Now if database server is running in container_name_2 then
    we container_name_1 can talk to it something like this
    snippet: mongoose.connect("mongodb://container_name_2:27017/swfavorites", {}, () => {})

    Note- Docker will not replace your source code it simply detects outgoing requests
    and resolve the IP address for such requests

33. Docker Network Drivers
    Docker Networks actually support different kinds of "Drivers" which influence the behavior of the Network.
    The default driver is the "bridge" driver - it provides the behavior 
    shown in this module (i.e. Containers can find each other by name if they are in the same Network).
    The driver can be set when a Network is created, simply by adding the --driver option.
    snippet: docker network create --driver bridge my-net
    Of course, if you want to use the "bridge" driver, 
    you can simply omit the entire option since "bridge" is the default anyways.
    Docker also supports these alternative drivers - though you will use the "bridge" driver in most cases:
    host: For standalone containers, isolation between container 
    and host system is removed (i.e. they share localhost as a network)
    overlay: Multiple Docker daemons (i.e. Docker running on different machines) are 
    able to connect with each other. Only works in "Swarm" mode which is a dated / almost 
    deprecated way of connecting multiple containers
    macvlan: You can set a custom MAC address to a container - this address can 
    then be used for communication with that container
    none: All networking is disabled.
    Third-party plugins: You can install third-party plugins which then may add all kinds of behaviors and functionalities
    As mentioned, the "bridge" driver makes most sense in the vast majority of scenarios.

34. docker-compose
    docker compose is a tool that allows you to replace
    docker-build and docker-run commands with just one 
    configuration file and then a set of orchestration commands
    to start all those services, all the containers at once 
    and build all necessary images, if it should be required
    and then you can also use one command to stop everything 
    and bring every thing down
    this approach is great for multicontainer application
    snippet: docker-compose.yaml
        version: 3.8 (version helps us to tell docker what features we are going to use)
        services:    (services are basically containers)
            mongodb:
                image: 'mongo'             
                # image we want to use for this container
                volumes: 
                    - data:/data/db        
                    #for every volume add a dash
                environment:
                    MONGO_USER: root
                    MONGO_PASSWORD: secret or - MONGO_PASSWORD=secret
                env_file: 
                    - ./mongo.env
                # we don't have to specify --rm flag by default container will be removed on shutdown)
                container_name: mongodb
            backend:
                build: ./backend    
                # or 
                # build: 
                #    context: ./backend   (set the context in which docker file will be built)
                #    dockerfile: Dockerfile
                #    args:
                #       some-args: 1 
                # docker compose also  replaces image-build step instead of specifying finished 
                # image we can give docker compose all the information it needs to build an image
                # now the build option wants to know where to find the docker file that should be built

                volumes:
                    - logs:/app/logs
                    - ./backend:/app
                    - /app/node_modules

                ports: 
                    - '80:80'

                depends_on:    
                    - mongodb
                # we can specify the list of other containers
                # on which this container depends on so that docker can first 
                # bring up those container and then this one
                
                # docker-compose creates a default network and adds all containers to that network

            frontend:
                build: ./frontend
                ports: 
                    - '3000:3000'
                stdin_open: true
                # to let the docker know that this service needs a open input connection 
                tty: true
                # tty flag to attach terminal to standard input connection
                # both of the above two keys are equivalent to -it flag
            volumes:      (all the named volumes used above have to be listed here)
                data:     (no value needs to be assigned just list them here)
                logs:
            

                 
35. To start services mentioned in docker-compose
    and it will not just start the containers 
    it will also pull and build all the images that might be required
    snippet: docker-compose up (build + run)
             docker-compose build (build)

36. If we want to shut and remove all containers
    it does not deletes volumes though if you want to do that add -v flag
    snippet: docker-compose down

37. Utility containers
    these containers only have certain environment in them.
    let's say a nodejs or php environment something like that.
    The idea is that they don't start an application
    when you run them but instead you run them in conjunction with some 
    command specified by you to then execute certain task 
    snippet: docker run mynpm init
    this is special syntax adding a extra command after image name
    snippet: 
    docker run -it -d node
    docker exec -it container_name npm init

    this can be useful if want to read logs from running containers

38. We can setup ec2 instance on aws and use ssh(secure shell)
    to communicate to this remote machine

39. Pushing our local image to the cloud
    we can prepare our image locally and then just get the finished
    image onto the remote host to then run it as a container
    and we can use docker hub for that.
    we can build our image locally then push it to docker hub
    and then pull it from remote server to then run it on the remote server.
    first create new repository on docker hub
    now we can push to this repository by creating an image on our local machine
    snippet:
    docker build local_image_name . 
    docker tag local_image_name kuldeepkashyap/temp-repo
    docker push kuldeepkashyap/temp-repo

40. Now that image is pushed we have to now and run it on remote machine.
    snippet: docker run -d --rm kuldeepkashyap/temp-repo
    we didn't need to configure node.js environment on the server
    but we just installed docker and used it run the locally build image.

41. To do any changes in container running on remote machine,
    do the changes on your local and rebuild the image and add appropriate tag,
    and push it to docker hub.
    Now on the remote machine stop the currently running container, 
    pull the updated image and run it.
    snippet: 
    docker build local_image_name . 
    docker tag local_image_name kuldeepkashyap/temp-repo
    docker push kuldeepkashyap/temp-repo

    docker stop container_name
    docker pull kuldeepkashyap/temp-repo
    docker run -d --rm kuldeepkashyap/temp-repo

42. Disadvantages of current approach
    you fully own ec2 machine, you are fully responsible for it and also for its security
    you are responsible for configuration of this remote machine
    you have to take care if it's powerful enough
    you have to replace it with more powerful one if you get more traffic.
    you also have to managed in which network it is.
    the security groups and therefore firewall
    solution: Managed remote machines(AWS ECS) (elastic container service)

43. Configuring ECS has 4 steps
    container definition: configure all the containers that our application going to use
    task definition: a blueprint for your application here we can tell aws how it should launch our Container
        not how it should execute docker run but how the server on which it runs this should be configured
        here we are telling aws how it should execute our containers and which environment it should
        generally setup for them.
        fargate(default): which lauches it in a serverless-mode which means aws does not create an ec2 instance
        but instead it stores your container and your run settings and whenever there is a request that requires 
        the container to do something it starts the container up and handles that request and then stops it again
        which ofcourse is cost effective because you pay for the time your container is executing.
    service: controls the configured application should be executed 
            ex - you can add load balancer here
    cluster: a overall network in which services is run 
    
44. EFS(elastic file system)
    EFS allows us to attach a file system to our containers
    we have to configure volume or bind mount that container is using EFS.
    EFS allows us to attach hard drives to containers which will survive 
    even if these containers are redeployed.

45. Multi-stage builds
    allows you to have one dockerfile that define multiple build steps
    or setup steps so called stages inside of that file
    stages can copy results(created files and folders) from each other 
    so we can have one stage to create the optimized files
    and another stage to serve them.
    we can either build the complete image or select individual stages
    Every "FROM" instruction creates a new stage in your docker file 
    we must have only one base image but we can switch that image
    in that case the previous steps will be discarded and we will
    switch to new base image.
    but there is a special keyword("AS") if we don't want our changes to be discarded.
    snippet:
    FROM node:14-alpine as frontendApp
    WORKDIR /app
    COPY package.json .
    RUN npm install
    COPY . .
    RUN npm build

    FROM nginx:stable-alpine
    COPY --from=frontendApp /app/build /usr/share/nginx/html
    EXPOSE 80
    CMD ["nginx", "-g", "daemon off;"]

46. we can also stop the build at particular stage using --target flag.
    this is useful in more complex projects
    snippet: docker build --target frontendApp -f frontend/Dockerfile.prod ./frontend
    this will stop the build at "RUN npm build"

47. 3 ways to configure aws service
    -> by using visual user interface 
    -> by using command line tool( ecs-cli )
    -> by using configuration files(YAML) which aws reads and make modifications


Kubernetes

48. Kubernetes
    is more like a framework(a collection of tools), a collection of concepts, a standard
    which will help us with container orchestration and large scale deployments
    independent of the cloud provider we might be using

49. Problems with current deployment process
    -> we have to configure ec2 on our own and we have to take care of OS update and security
    -> container might crash/go down and need to be replaced with a new container
    -> we might need more container instances upon traffic spikes
    -> incoming traffic should be distributed evenly b/w container instances
    
50. Advantages of using ECS
    -> ECS monitor health of our containers and restarts them when they crash
    -> ECS also helps us with autoscaling
    -> ECS also have a tool called loadbalancer to distribute traffic evenly
    

51. Disadvantages of using ECS 
    if we use a specific cloud service by some cloud provider
    we are basically locked into that service 
    which means if we want to switch to another service provided by another cloud provider
    the config files we wrote for aws ecs won't work there becasue that is a aws specific config
    and we have to learn first about ECS and if want to switch learn about another service
    and that's where kubernetes can help us

52. why kubernetes?
    kubernetes can help us with the problem mentioned above
    with kubernetes we have a way of defining our deployments, 
    our scaling of containers 
    and how containers should be monitored and replaced if they fail
    we have a way of defining all of that independent of the cloud service we are using
    because kubernetes is an open source standard and de-facto standard for managing container deployments
    and for orchestrating containers and it can help us with tasks like automatic deployment
    with scaling and load balancing and with managing our deployments
    kubernetes simply allows us to write down a kubernetes configuration
    where we define our desired deployment which container we want to deploy
    how many instances, if it should scale up, if they should replaced and so on
    and this configuration will work with any cloud provider as long as it supports kubernetes
    and even if does not support it we can install some kubernetes software on any machine you own
    so with kubernetes we have a standardized way of describing the to-be-managed resources
    of the kubernetes cluster.
    and what's even better is the fact that we can actually merge certain cloud provider-specific options
    into this config file so that if some cloud provider would require additional configuration
    you could just add that configuration into that main file.
    
53. Kubernetes - having this standarized way of describing deployments
    we can think of kubernetes as docker-compose for multiple machines
    because docker-compose helps us manage multi container projects on our local machine

54. Pod
    In kubernetes world a container is managed by pod
    you can think of a pod as the smallest possible unit in the kubernetes world
    which you can define in some configuration file for kubernetes to create
    and pod can hold a single or multiple containers and their resources(volumes, IP, run config)
    but the smallest unit possible is simply one pod which is responsible for running a container.
    kubernetes can create or delete pods.
   
55. Worker node
    the pod with the container inside of it then itself runs on a so called worker node.
    worker node run the containers of the application.
    you can think of worker node as your machines/ virtual instances.
    A worker node is simply a machine, a computer somewhere with a certain amount of cpu and memory
    and on that machine, you can run your pods. 
    And you can have more than one pod running on one of the same worker node.
    worker node is managed by master node(to be precise what's happening on the worker node like creating a pod).
    Pods themselves also managed by master node 
    we can have more than one running pod on a worker node. now that can be copy of another pod,
    incase you are scaling it up and you want to have multiple instances off one and the same 
    container up and running to distribute incoming traffic. but it can also be a pod with a 
    totally different container inside of it to do a totally different task because worker node is not task specific.
    software needed on worker machine: 
    docker
    kubelet - which is basically  the communication device b/w worker node and master node
    kube-proxy - which is responsible for handling incoming and outgoing traffic to insure that 
                everything is working as desired and only allowed traffic is able to reach the pods.
                and only allowed traffice is able to leave the worker node.
    
56. Proxy
    Another tool kubernetes sets up for you in the end on a worker node to control the network traffic
    of the pods on that worker node. so basically to control whether pods can reach the internet 
    and how these pods and therefore the container inside them can be reached from outside world.
    If you are running a web app in a container in such a pod for example.
    the proxy needs to be configured such that traffic from the outside world from your users 
    is able to reach this container.

57. Master node
    we use kubernetes to dynamically add/remove containers and therefore pods as traffic increases or decreases.
    these pods are automatically distributed by kubernetes across all worker nodes so you can have different
    and equal containers running on multiple worker nodes to distribute workload evenly.
    now all these worker nodes and the pods and containers running on them needs to be controlled somehow.
    someone needs to create and start these containers and pods and someone needs to replace or shut them down
    if they are failing or not needed anymore and that's done by the master node(specially by the control plane).
    services running on master machine: 
    API server - simple service running on master node machine which is counterpoint for kublet services
                running on worker nodes. so this is for communication b/w worker node and master node.
    scheduler - responsible for watching our pods and choosing worker nodes and choosing worker nodes
                on which new pods should wbe created on, in case new pod needs to be addded 
                because a new pod got unhealthy and went down or because of scaling, we needed to create
                a new pod. so this is responsible for telling the API server what to tell the worker nodes.
    kube-controller-manager - watches and controls the worker nodes overall, and which is responsible for
                ensuring that we got the correct number of pods up and running and therefore, it works 
                closely together with scheduler and API server.
    cloud-controller-manager - this is a specific version of kube-controller-manager, which is doing the same thing
                but which is cloud-provider specific and which actually then knows how to tell that cloud provider,
                what to do. so this cloud-controller-manager just translates instructions to AWS.
                Big cloud-provider already got services which take all that away from you(you might not need to 
                manually create the master and worker nodes and install all these services) so you just need
                to provide your kubernetes config and they do all the heavy lifting.

58. Control plane
    This is basically the control center which then interacts with worker nodes to control them.
    so when working with kubernetes we don't directly interact with worker nodes or pods though we could do that,
    instead we let kubernetes and this control plane do that heavy lifting and you as a developer
    just define the desired end state kubernetes should take into account.
    so this master nodes is simply another server(renote machine) which has this control plane running on it,
    which then is responsible for interacting with the worker nodes and the pods running on them.
    now theoretically, you can have one machine which acts as both the master and only worker node,
    but for bigger deployments ofcourse you will have a master node which itself might actually be split
    across multiple machines to ensure high availability and your worker nodes would then be other instances
    other machine independent from the master node so that if one worker node goes down,
    your master node doesn't go down together with it.

59. Cluster
    all of the things we discussed above will form a cluster of your master and worker nodes
    and therefore one network in which all these different parts are connected and then your master
    node is able to send instructions to cloud provider API, to tell that cloud provider to create it's
    cloud provider specific resources to replicate this desired big picture on that cloud provider

60. Services
    A logical set(group) of Pods with a unique, Pod- and container- independent IP address.
    services are important to reach our pods and containers running inside them.
    it is just a term for exposing certain pods to the outside world.

60. What Kubernetes will do for you?
    we need to provide a certain environment in which kubernetes is able to run.
    you as a developer/administrator have to do is you have to create and setup the cluster
    and the worker node and the master node on which kubernetes is then installed and does its thing.
    you have to install all the kubernetes software on these node so on these machines which are 
    part of your cluster and you have to configure all these things kubernetes needs 
    depending on the cloud provider you might be using you also might need to set up extra resources
    your kubernetes cluster might need eventually(like load balancer or file system).

61. What are things you need to do in advance in order to then use kubernetes?
    kubernetes will use the resources discussed above, once it is installed and set up.
    kubernetes will manage these pods, it will create them, automatically distribute them 
    to take advantage of the available resources. it will monitor and restart them if they fail.
    it will manage the running deployment, it will ensure that it runs smoothly and that it stays up.
    it will start the containers for you.

62. Kubermatic 
    is a tool which can help us create resources on cloud like AWS.
    if you don't wanna create everything manually.
    AWS also has ELS(Elastic kubernetes service) which also helps us to create resources.

63. Kubectl (kube-control tool)
    is used to send instructions to the cluster for ex- to create a new deployment
    delete a deployment or change our running deployment.
    with the kube-control tool you as the developer and administrator
    are able to send instructions to the master node whatever it needs to do with the worker nodes.

64. Minikube
    to setup dummy cluster locally we can use a tool called Minikube.

65. To create cluster locally:
    brew install minikube
    brew install kubectl
    minikube start --driver=docker 
    minikube status 
    minikube dashboard 
    kubectl create deployment first-app --image=kuldeepkashyap/kube-first-app
    kubectl get deployments
    kubectl get pods 
     
66. Kubernetes Objects
    kubernetes works with so called objects
    kubernetes knows a couple of objects: pods, deployments, servies, volumes etc.
    the idea behind these objects is that you can create such an object by executing
    a certain command and then kubernetes will take that object created by you and 
    then it will do something based on the instructions encoded in that object
    and these objects can actually be created in two different ways, imperatively
    or declaratively. 

67. Pod Object 
    If you wanna tell kubernetes that it should create a pod, 
    run a container and do that on some worker node in the cluster,
    you can do that by creating a pod object in code or with help of a command,
    and sending that object to kubernetes, 
    -> pods are not just to run a container
    -> though they can also hold shared resources which these container might utilize
        most importantly volumes.
    -> pods are part of the cluster therefore it can communicate with our pods 
        or outside world. but by default, a pod has a cluster internal ip address
        now that is something which we'll be able to change or we'll be able to 
        communicate with pods nonetheless, even from outside the cluster.
        but the internal ip address can be used to send requests to that pod,
        and therefore to the containers running in the pod.
        now if you have multiple container in one of the same pod, 
        then these containers will be able to communicate with each other 
        by using a localhost address. pods are similar to tasks in AWS ESC.
    note: pods are designed to be ephemeral: kubernetes will start, stop, and replace them as needed.
    which basically means that they don't persist. if a pods is replaced or removed by kubernetes
    all the resources in the pod for ex- data stored and created by a container is lost.

68. Deployment Object
    -> we don't create pod container on our own instead we create deployment object 
    which you then give instructions about the number of pods and containers it should create 
    and manage for you. the deployment object is able to control one or multiple pods.
    the core philosophy behind this deployment object, which under the hood is controller object,
    is that we set a desired target state for ex- we have 2 pods with this container up and running
    and then kubernetes will do everything it needs to do to reach this target state.
    so we can can define what we want and kubernetes will get us there.
    and the advantage of this is that the pod objects are then created by kubernetes and 
    the pods are therefore created by kubernetes, the containers are started by kubernetes 
    and kubernetes will also placed these pods on worker nodes which have sufficient memory
    and cpu capacity for these new pods. so we don't manually have to pick a remote machine
    and place our pods on there.

    -> Another advantage of using deployment object is that we can also pause or delete
    deployments and rolled back. so if mess something up and our application fails,
    we don't have to rush to change our code and fix the error and replace the failing pod with a new one.
    instead we can just roll back the failing deployment and go back to the previous deployment which worked.

    -> Another feature is that the deployements can also be scaled.
    we can tell kubernetes that we now want to have more pods or less pods and we can also use a 
    feature called autoscaling where we can set certain metrics for ex incoming traffic and cpu utilization
    and when these metrics are exceeded kubernetes automatically creates more pods and therefore more 
    running container instances.

69. Commands and what's happening behind the scenes.
    -> snippet: kubectl create deployment --image=kuldeepkashyap/kube-first-app
    this creates a deployment object and then automatically sends it to our kubernetes cluster(to master node)
    so on master node the scheduler analyses the currently running pods and it finds the best node for the 
    newly created pod and then this newly created pod will be moved to one of our worker node
    and on that worker node we have kublet service which now manages the pod, starts the container in the pod,
    and basically also monitors this pod and checks its health now this pod continer based on the image which we
    specified in --image flag which we created when we created deployment object. 

70. Service Object
    to reach a pod and the container running in a pod, we need a service.
    Because service object is another object kubernetes knows, which is responsible for
    exposing pods to other pods in the cluster or to visitors outside the cluster.
    pods already have an internal IP address by default.
    but there are 2 problems with this internal ip address,
    for one we can't use that to access the pod from outside the cluster, which is a problem.
    and the second problem is that it will change whenever a pod is replaced.
    so we can't reply on pod keeping its IP address, therefore the IP address is not really a great tool 
    for communicating with the pod even if it would be cluster internal.
    now service groups pods together and gives them a shared address(a shared IP address)
    and this address then won't change so you can use a service to move multiple pods 
    into that service and make them reachable on that service unchangable IP address
    and even better than that you can also tell the service to expose this address
    which doesn't change not just inside of the cluster but outside of the cluster
    so that pods can be accessed from outside of the cluster.
    the default is that it's internal only but with a service, we can overwrite this.

71. Expose a created by deployment by creating such a service 
    snippet: kubectl expose deployment first-app --type=LoadBalancer --port=8080
             kubectl get services
             minikube service first-app (it for local only to map special port to an IP which we can reach from our local machine)
    --type=ClusterIP this is a default type which means it will only be reachable from inside the cluster
    --type=NodePort means this deployment should be exposed with help of the IP address of the worker node on which it is running
    --type=LoadBalancer it utilizes a load balancer which has to exists in the insfrastructure on which our cluster runs 
        and then this load balancer will actually generate a unique address for this service and it will not just do that,
        as the name implies it will also evenly distribute incoming traffic across all pods, which are part of this service.

72. Scaling pods(create multiple instances of the pod)
    snippet: kubectl scale deployment/first-app --replicas=3

73. Updating deployments
    snippet: 
    docker build -t kuldeepkashyap/kube-first-app:2 .
    docker push kuldeepkashyap/kube-first-app:2
    kubectl set image deployment/first-app kube-first-app=kuldeepkashyap/kube-first-app:2
    -> to update the image of the currently deployed container(i.e kube-first-app) container_name=image_name

74. To check the current deployment status.
    snippet: kubectl rollout status deployment/first-app

75. Deployment rolllback.
    snippet: kubectl rollout undo deployment/first-app

76. Deployment history
    snippet: kubectl rollout history deployment/first-app

77. Delete deployment and service.
    snippet: kubectl delete service first-app
             kubectl delete deployment first-app
            
78. The approach discussed above i.e by using commands is Imperative approach.
    there is another approach called Declarative approach,
    in which we can create k8s config file.
    Declarative approach is like running docker run command.
    Imperative approach is like runnig docker-compose up.

79. Declarative approach
    -> create a yaml file(any name) for k8s config.
    snippet: deployment.yaml
    apiVersion: apps/v1
    # no matter which kind of resource we are configuring
    # if it is a deployment or service, whatever it is we need to specify an apiVersion
    # k8s is still under active development so there are multiple apiVersions available

    kind: Deployment
    # to let k8s know what kind of k8s object we want to create

    metadata: 
        name: second-app-deployment
    # this is not some random data that we come up with but it includes crucial things like the name
    # of the object that we are creating 

    spec: 
    # this is the meat of this object because here we define how this deployment should be configured.
    # we add pieces of information about this deployment
    # for ex- the number of pod instances we wanna have as a default we don't add it the default value is 1.

        replicas: 3
        # how many pods we want to be created to start with.
        # we can even set it 0, if we don't want anypods to be launched initially.

        selector: 
            matchLabels: 
                app: second-app
                tier: backend
            # here we add key-value pair of the pod labels we wanna match with this deployment.
            # what this means is, we got a template for this deployment and therefore it should be
            # obvious which pods are created by this deployment. the pods defined in this template
            # will be created when this deployment is applied but deployments are dynamic objects
            # in the k8s world. for ex if you scale up the number of pods after a deployment has been 
            # created these new pods which are created will still automatically be managed by already
            # existing deployment so deployment continuously watches all the pods which are out there 
            # and sees if there are any pods it should control. and it selects the to be controlled pods 
            # with a so called selector. now there are different types of selectors here for deployment
            # we can use two different kinds of selecting. we can match labels and we can match expressions.
            matchExpressions:
                - {key: app, operator: In, values: [second-app, first-app]}
                # operator in means app has one of the value specificied in values array


        template: 
        # here we will define the pods that should be created as part of this deployment

            metadata:
            # because pod is a new object in the k8s world.

                lables:
                # we can add any key-value pair in labels 
                    app: second-app

            spec: 
            # how this pod should be configured. 
            # we did specification before but that was the specification
            # for the overall deployment

                containers: 
                # here we can define a list of containers or single container
                # to defien the elements of the list in yaml we use dash(-)
                    - name: second-node
                      image: kuldeepkashyap/kube-first-app:2
                      imagePullPolicy: Always
                      # always pull the latest image
                      livenessProbe: 
                      # How k8s check whether your pods and containers in the pods are healthy or not?
                      # periodic probe of container livenessProbe. container will be restarted if the probe fails.
                        httpGet: 
                            path: /api/health
                            port: 8080
                        periodSeconds: 10
                        initialDelaySeconds: 5

80. how can we now apply the above deployment?
    how can we make the cluster aware of it? and have it create that deployment and pod?
    snippet: kubectl apply -f=deployment.yaml
    "apply" command simply simply applies a config file to the connected cluster

81. how can we apply the service?
    snippet:
    kubectl apply -f=service.yaml 
    minikube service service_name (after the service is created expose it using minikube)


82. Deleting resources
    snippet: kubectl delete -f=deployment.yaml, service.yaml
   
83. 
    


